# -*- coding: utf-8 -*-
"""lab2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13z8qDDRIkf0P_ymaWCuKe0S65oghZy-z
"""

# ============================================================
# PERMUTED MNIST â€“ Measuring Catastrophic Forgetting (TF2)
# Full Colab/Script Version with TBWT & CBWT Visualization
# Author: Sai Charan Kalagoni
# ============================================================

import os, time, random
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Try seaborn for nicer heatmaps; fall back gracefully if not present
try:
    import seaborn as sns
    _HAS_SNS = True
except Exception:
    _HAS_SNS = False

# -------------------------------
# Configuration (edit as needed)
# -------------------------------
class Args:
    num_tasks = 5            # set to 10 for the full assignment
    depth = 3                # hidden layers: choose 2, 3, or 4
    hidden_units = 256
    dropout = 0.3            # must be <= 0.5 per instructions
    seed = 3949              # unique seed (e.g., last 4 of NAU email id)
    initial_epochs = 5       # use 50 for full assignment
    epochs_per_task = 3      # use 20 for full assignment
    batch_size = 64
    lr = 1e-3
    loss = 'nll'             # one of: 'nll', 'l1', 'l2', 'l1+l2'
    opt = 'adam'             # one of: 'sgd', 'adam', 'rmsprop'
    outdir = './results_sai_charan_kalagoni'
args = Args()

os.makedirs(args.outdir, exist_ok=True)
tf.keras.utils.set_random_seed(args.seed)
np.random.seed(args.seed)
random.seed(args.seed)

# -------------------------------
# Dataset
# -------------------------------
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32")  / 255.0
x_train = x_train.reshape((-1, 784))
x_test  = x_test.reshape((-1, 784))

def make_permuted_tasks(n, seed):
    """Seeded permutations for reproducibility."""
    rng = np.random.RandomState(seed)
    return [rng.permutation(784) for _ in range(n)]

tasks = make_permuted_tasks(args.num_tasks, args.seed)

# -------------------------------
# Model Definition
# -------------------------------
def build_mlp(input_dim, depth, hidden_units, dropout_rate):
    model = keras.Sequential(name=f"mlp_d{depth}_h{hidden_units}_dr{dropout_rate}")
    model.add(layers.InputLayer(input_shape=(input_dim,)))
    for _ in range(depth):
        model.add(layers.Dense(hidden_units, activation='relu', kernel_initializer='he_normal'))
        if dropout_rate and dropout_rate > 0:
            model.add(layers.Dropout(dropout_rate))
    # Use softmax output; NLL uses CE with from_logits=False
    model.add(layers.Dense(10, activation='softmax'))
    return model

def one_hot(y, depth=10):
    return tf.one_hot(tf.cast(y, tf.int32), depth=depth)

def get_loss_fn(name):
    # NLL: cross-entropy on probabilities
    if name == 'nll':
        return keras.losses.SparseCategoricalCrossentropy(from_logits=False)

    # L1/L2: compare probs vs one-hot target
    if name == 'l1':
        def l1_loss(y_true, y_pred):
            y_true_oh = one_hot(y_true, depth=10)
            return tf.reduce_mean(tf.reduce_sum(tf.abs(y_true_oh - y_pred), axis=-1))
        return l1_loss

    if name == 'l2':
        def l2_loss(y_true, y_pred):
            y_true_oh = one_hot(y_true, depth=10)
            return tf.reduce_mean(tf.reduce_sum(tf.square(y_true_oh - y_pred), axis=-1))
        return l2_loss

    if name == 'l1+l2':
        def l1l2_loss(y_true, y_pred):
            y_true_oh = one_hot(y_true, depth=10)
            diff = y_true_oh - y_pred
            l1 = tf.reduce_mean(tf.reduce_sum(tf.abs(diff), axis=-1))
            l2 = tf.reduce_mean(tf.reduce_sum(tf.square(diff), axis=-1))
            return l1 + l2
        return l1l2_loss

    raise ValueError(f"Unknown loss: {name}")

def get_optimizer(name, lr):
    if name == 'sgd':     return keras.optimizers.SGD(learning_rate=lr)
    if name == 'adam':    return keras.optimizers.Adam(learning_rate=lr)
    if name == 'rmsprop': return keras.optimizers.RMSprop(learning_rate=lr)
    raise ValueError(f"Unknown optimizer: {name}")

# -------------------------------
# Metrics
# -------------------------------
def compute_ACC_paper(R):
    """ACC per Lopez-Paz et al.: mean of the last row (after all tasks)."""
    return float(np.mean(R[-1, :])) if R.size else 0.0

def compute_ACC_diag(R):
    """Legacy diagonal mean (kept for reference)."""
    n = min(R.shape[0], R.shape[1])
    return float(np.mean(np.diag(R[:n, :n]))) if n > 0 else 0.0

def compute_BWT(R):
    """BWT = 1/(T-1) * sum_{i=1..T-1} (R[T-1,i] - R[i,i])."""
    T = R.shape[0]
    if T <= 1: return 0.0
    diffs = [R[-1, i] - R[i, i] for i in range(T - 1)]
    return float(np.mean(diffs))

def compute_TBWT(R):
    """Average backward transfer across trajectory (your original version)."""
    n = R.shape[0]
    s, c = 0.0, 0
    for i in range(1, n):
        for j in range(i):
            s += R[i, j] - R[j, j]
            c += 1
    return (s / c) if c > 0 else 0.0

def compute_CBWT(R):
    """Average of (R[i,i] - R[0,0]) for i=1..T-1 (your original version)."""
    n = R.shape[0]
    if n <= 1: return 0.0
    return float(np.mean([R[i, i] - R[0, 0] for i in range(1, n)]))

# -------------------------------
# Training Loop
# -------------------------------
def run_experiment():
    model = build_mlp(784, args.depth, args.hidden_units, args.dropout)
    model.compile(optimizer=get_optimizer(args.opt, args.lr),
                  loss=get_loss_fn(args.loss), metrics=['accuracy'])

    # R[t, j] = accuracy on task j after finishing training task t
    R = np.zeros((args.num_tasks, args.num_tasks), dtype=np.float32)

    for t, perm in enumerate(tasks):
        print(f"\n===== Training Task {t+1}/{args.num_tasks} =====")
        x_train_p = x_train[:, perm]
        x_test_p  = x_test[:, perm]
        epochs = args.initial_epochs if t == 0 else args.epochs_per_task

        model.fit(x_train_p, y_train,
                  validation_data=(x_test_p, y_test),
                  epochs=epochs, batch_size=args.batch_size, verbose=2)

        # Evaluate on all tasks learned so far
        for j, eval_perm in enumerate(tasks[:t+1]):
            x_eval = x_test[:, eval_perm]
            _, acc = model.evaluate(x_eval, y_test, verbose=0)
            R[t, j] = acc

    return R

# -------------------------------
# Run & Evaluate
# -------------------------------
start = time.time()
R = run_experiment()
print(f"\nTraining finished in {(time.time() - start):.2f}s")

ACC_paper = compute_ACC_paper(R)
ACC_diag  = compute_ACC_diag(R)
BWT       = compute_BWT(R)
TBWT      = compute_TBWT(R)
CBWT      = compute_CBWT(R)

print("\n===== Final Metrics (Sai Charan Kalagoni) =====")
print(f"ACC (paper, last-row mean): {ACC_paper:.4f}")
print(f"ACC (diag mean, reference): {ACC_diag:.4f}")
print(f"BWT : {BWT:.4f}")
print(f"TBWT: {TBWT:.4f}")
print(f"CBWT: {CBWT:.4f}")

# -------------------------------
# Visualization
# -------------------------------
plt.figure(figsize=(7, 6))
if _HAS_SNS:
    sns.heatmap(R, annot=True, cmap="YlGnBu", fmt=".3f")
else:
    plt.imshow(R, aspect='auto')
    plt.colorbar()
    for i in range(R.shape[0]):
        for j in range(R.shape[1]):
            if R[i, j] > 0:
                plt.text(j, i, f"{R[i, j]:.3f}", ha='center', va='center')
plt.title("Accuracy Matrix R[i, j] - Sai Charan Kalagoni")
plt.xlabel("Evaluated Task (j)")
plt.ylabel("After Training Task (i)")
plt.tight_layout()
plt.savefig(os.path.join(args.outdir, "accuracy_matrix_heatmap.png"))
plt.show()

# Forgetting curves: accuracy of each task over time
plt.figure(figsize=(7, 5))
for j in range(args.num_tasks):
    plt.plot(range(1, args.num_tasks + 1), R[:, j], marker='o', label=f'Task {j+1}')
plt.xlabel("After Training Task")
plt.ylabel("Accuracy")
plt.title("Forgetting Curve - Permuted MNIST (Sai Charan Kalagoni)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig(os.path.join(args.outdir, "forgetting_curve.png"))
plt.show()

# TBWT/CBWT comparison
plt.figure(figsize=(5, 4))
plt.bar(["TBWT", "CBWT"], [TBWT, CBWT])
plt.title("TBWT vs CBWT (Sai Charan Kalagoni)")
plt.ylabel("Score")
plt.ylim(-1, 1)
plt.grid(axis="y")
plt.tight_layout()
plt.savefig(os.path.join(args.outdir, "tbwt_cbwt_bar.png"))
plt.show()

# -------------------------------
# Save Outputs
# -------------------------------
np.save(os.path.join(args.outdir, "accuracy_matrix.npy"), R)
with open(os.path.join(args.outdir, "metrics.txt"), "w") as f:
    f.write("Author: Sai Charan Kalagoni\n")
    f.write(f"ACC_paper : {ACC_paper:.4f}\n")
    f.write(f"ACC_diag  : {ACC_diag:.4f}\n")
    f.write(f"BWT       : {BWT:.4f}\n")
    f.write(f"TBWT      : {TBWT:.4f}\n")
    f.write(f"CBWT      : {CBWT:.4f}\n")

print(f"\nResults and plots saved under {args.outdir}")

# -------------------------------
# Tips to run full assignment
# -------------------------------
# args.num_tasks = 10
# args.initial_epochs = 50
# args.epochs_per_task = 20
# Re-run the cell/script after changing the above.